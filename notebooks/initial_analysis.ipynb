{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774648a3",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:3em;\">Analysis of Fitting Surfaces</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a20aea",
   "metadata": {},
   "source": [
    "# Narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5daf71",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2988bd74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.LTIModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-88201de90c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfaceAnalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSurfaceAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLTIModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLTIModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon_python\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_python\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympyUtil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.LTIModel'"
     ]
    }
   ],
   "source": [
    "from src.surfaceAnalyzer import SurfaceAnalyzer\n",
    "from src.LTIModel import LTIModel\n",
    "from common_python.common_python.sympy import sympyUtil as su\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import tellurium as te\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import sympy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684438d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRMSE = \"nrmse\"  # Normalized root of the mean square error (residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea85346",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/home/ubuntu/FittingSurface/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcd301",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(MODEL_DIR, \"Jana_WolfGlycolysis.antimony\")\n",
    "with open(path, \"r\") as fd:\n",
    "    GMODEL = fd.readlines()\n",
    "GMODEL = \"\\n\".join(GMODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee244f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GMODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cd32b",
   "metadata": {},
   "source": [
    "# One Species (First Order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654eee4",
   "metadata": {},
   "source": [
    "## Numerical Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab2ba3",
   "metadata": {},
   "source": [
    "Consider the pathway $X0 \\xrightarrow{k_1} x \\xrightarrow{k_2} X_1$, where $X_0$ and $X1$ have fixed concentrations and reactions kinetics\n",
    "are mass action with kinetic constants $k_i$.\n",
    "Then, $\\frac{dx}{dt} = k_1 X_0 - k_2 x$.\n",
    "We want to explore the fitting surface for $k_1, K_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"\"\"\n",
    "\n",
    "J1: $X0 -> x; k1*X0\n",
    "J2: x -> $X1; k2*x\n",
    "\n",
    "X0 = 1\n",
    "x = 0\n",
    "k1 = 1\n",
    "k2 = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER_DCT = {\"k1\": 1, \"k2\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2299bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = te.loada(MODEL)\n",
    "trueData = rr.simulate()\n",
    "rr.plot(trueData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SurfaceAnalyzer(MODEL, PARAMETER_DCT)\n",
    "scales = [10, 2, 0.1, 0.03]\n",
    "k2s = [0.5, 1.0, 2.0]\n",
    "k2s = [1.0]\n",
    "for scale in scales:\n",
    "    for k2 in k2s:\n",
    "        parameterDct = dict(PARAMETER_DCT)\n",
    "        parameterDct[\"k2\"] = k2\n",
    "        analyzer = SurfaceAnalyzer(MODEL, parameterDct)\n",
    "        analyzer.runExperiments(0.5, 50)\n",
    "        title = \"k2: %2.2f, scale: %2.4f\" % (k2, scale)\n",
    "        analyzer.plotSurface(scale=scale, title=title, xlim=[0.5, 1.5], ylim=[0.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bba504",
   "metadata": {},
   "source": [
    "**observations**\n",
    "1. Convex surface\n",
    "1. Lower values on line with slope $k_1 / k_2$.\n",
    "1. Steeper slope for minima for $k_2$ compared with $k_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da6acb",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc6cf5",
   "metadata": {},
   "source": [
    "**Result**: Parameter fitting for a one species system is a convex optimization.\n",
    "Given observational data $y(t)$ for a one species system and $t_{ss}$ such that\n",
    "$y(t) = x_{ss}$ for $t \\geq t_{ss}$.\n",
    "Let $x(t; k_1, k_2)$ be the system with parameters $k_1, k_2$ and\n",
    "$k_1^{\\star}, k_2^{\\star}$ such that $y(t) = x(t; k_1^{\\star}, k_2^{\\star})$.\n",
    "We claim that $k_1^{\\star}, k_2^{\\star}$ are the solution to the following convex optimization:\n",
    "minimize $\\sum_t \\left( y(t) - x(t; k_1, k_2) \\right) ^2$ such that $\\frac{k_1}{k_2} = x_{ss}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a69073",
   "metadata": {},
   "source": [
    "**Problem setup**\n",
    "1. The one species system with a constant input $X_0$ and initial condition $x(0)=0$ has the solution.\n",
    "$x(t) = X_0 \\frac{k_1}{k_2}(1 - e^{-k_2 t})$. **[Generalize to non-zero initial conditions]**\n",
    "\n",
    "1. The residuals are $r(t) =  y(t) - x(t; k_1, k_2) = x(t; k_1^{\\star}, k_1^{\\star}) - x(t; k_1, k_2)$\n",
    "\n",
    "1. The sum of squares of the residuals is $R_{SSQ} = \\int_0^{\\infty} r(t)^2 dt$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92620692",
   "metadata": {},
   "source": [
    "**Analysis for $\\frac{k_1}{k_2} = \\frac{k^{\\star}_1}{k^{\\star}_2}$**\n",
    "1. $r(t) =  A( -e^{-k_2 t} + e^{-k^{\\star}_2 t})$, where $A = X_0 \\frac{k_1^{\\star}}{k_2^{\\star}}$.\n",
    "\n",
    "1. $R_{SSQ} = A^2 \\int_0^{\\infty} \\left( e^{-2k_2 t} -2 e^{-(k_2 + k^{\\star}_2)t} + e^{-2k^{\\star}_2t} \\right)$\n",
    "\n",
    "1. And so, $R_{SSQ} = A^2 \\left( - \\frac{1}{2k_2} e^{-2k_2 t} +  \\frac{2}{k_2 + k^{\\star}_2} e^{-(k_2 + k^{\\star}_2)t} - \\frac{1}{2k^{\\star}_2} e^{-2k^{\\star}_2t} \\right) \\biggr\\vert_0^{\\infty}$\n",
    "\n",
    "1. $R_{SSQ} = A^2 \\left( \\frac{1}{2k_2} -  \\frac{2}{k_2 + k^{\\star}_2} + \\frac{1} {2 k^{\\star}_2}  \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04430964",
   "metadata": {},
   "source": [
    "In the sequel, we simplify notation by using\n",
    "$g(u; u^{\\star}) = \\frac{1}{2 u^{\\star}} -  \\frac{2}{u + u^{\\star}} + \\frac{1} {2 u}$, for $u, u^{\\star} > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac35ed",
   "metadata": {},
   "source": [
    "**Claim:** Given $g(u; u^{\\star})$ as defined above:\n",
    "1. $g(u; u^{\\star})$ is positive semidefinite.\n",
    "1. $g(u; u^{\\star}) = 0$ iff $u = u^{\\star}$\n",
    "1. $u^{\\star}$ is the solution to a convex optimization.\n",
    "\n",
    "**Proof:**\n",
    "The first two claims are established by the following:\n",
    "$\n",
    "g(u; u^{\\prime})  \\geq  0 \\equiv u(u + u^{\\star}) - 4 u u^{\\star} + u^{\\star} (u + u^{\\star}) \\geq 0\n",
    "\\equiv (u - u^{\\star}) ^ 2 \\geq 0$.\n",
    "The third claim follow from the fact that minimizing $g(u; u^{\\prime})$ is equivalent to solving a convex optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c05e2",
   "metadata": {},
   "source": [
    "**Claim**: $R_{SSQ}$ is convex in $k^{\\star}_2$. **[This may be unnecessary.]**\n",
    "\n",
    "1. Claim that $\\frac{\\partial T_{SE}}{\\partial k^{\\star}} \\geq 0$.\n",
    "\n",
    "1. Note that $\\frac{\\partial^2 R_{SSQ}}{\\partial^2 k^{\\star}_2} \n",
    "= A^2 \\left( \\frac{1}{k^3_2} -  \\frac{4 }{(k_2 + k^{\\star}_2)^3} + \\frac{1} {( k^{\\star}_2)^3}  \\right)$\n",
    "\n",
    "1. Suffices to show that $ \\left( \\frac{1}{k^3_2} -  \\frac{4 }{(k_2 + k^{\\star}_2)^3} + \\frac{1} {( k^{\\star}_2)^3}  \\right) \\geq 0$\n",
    "\n",
    "1. Equivalent to  $(k^{\\star}_2)^3  (k^{\\star}_2 + k_2)^3 -  4 k^3_2 (k^{\\star}_2)^3  + k^3_2 (k^{\\star}_2 + k_2)^3 \\ge0$.\n",
    "Let $x = k_2$ and $y = x^{\\star}_2$. Then, we have\n",
    "$x^3 (x + y)^3 -4 x^3 y^3 + y^3 (x + y)^3 \\geq 0$, where $x, y > 0$.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "T & = & x^3 (x^3 + 3x^2y + 3xy^2 + y^3) - 4 x^3 y^3 + y^3 (x^3 + 3x^2y + 3xy^2 + y^3)  \\\\\n",
    "& = & x^3 (x^3 + 3x^2y + 3xy^2) - 2 x^3 y^3 + y^3 ( 3x^2y + 3xy^2 + y^3)   \\\\\n",
    " & = & \\left( x^3 - y^3 \\right) ^2 + 6x^2y + 6 x y^2 \\\\\n",
    " & \\ge 0\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6362d62",
   "metadata": {},
   "source": [
    "# Two Species (Second Order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e5c3e",
   "metadata": {},
   "source": [
    "## Numerial Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27436",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d2402",
   "metadata": {},
   "source": [
    "Second order system: $y^{\\prime \\prime} + a y^{\\prime} + b y = u(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ba6be",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "LaPlace Transform: $Y(s) = \\frac{U(s)}{s^2 + a s + b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be317848",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. A key element in minimizing residuals over a longer time course is getting the correct steady state value. Need to constrain search for system parameters based on steady state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcfb9d",
   "metadata": {},
   "source": [
    "# Convex Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011a6e6",
   "metadata": {},
   "source": [
    "One direction is to establish a class of reaction networks for which fitting is a convex function.\n",
    "My hypothesis is that uni-reactant, feedforward networks have a convex objective function under\n",
    "the $l_2$ norm.\n",
    "This is non-trival to prove.\n",
    "So, a first step is to do a numerical evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d9a29",
   "metadata": {},
   "source": [
    "# Linear System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61edb61",
   "metadata": {},
   "source": [
    "## Convergence for Known A Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebb620",
   "metadata": {},
   "source": [
    "Consider a network that is an LTI system. For example, this holds for the following class of reaction networks:\n",
    "1. Mass action kinetics for at most one reactant.\n",
    "1. A constant is used in combination with exactly one chemical species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a6521",
   "metadata": {},
   "source": [
    "Let ${\\bf A}^{\\star}$ be the true matrix for the LTI system, and let ${\\bf A} ({\\bf k})$ be an estimate of the matrix in terms of the parameters\n",
    "${\\bf k} = (k_1, \\cdots, k_N)$.\n",
    "Our objective function to minimize is $g({\\bf k}) = | {\\bf A}^{\\star} - {\\bf A} ({\\bf k}) |^2$.\n",
    "It suffices to show that $\\frac {\\partial^2g (k_n)}{\\partial ^2 k_n} \\geq 0$ for each $k_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e5321",
   "metadata": {},
   "source": [
    "$\n",
    "g({k_n})  = | {\\bf A}^{\\star} - {\\bf A} ({k_n}) |^2\n",
    "$\n",
    "\n",
    "And so\n",
    "$\\frac{\\partial g(k_n)}{\\partial k_n} =\n",
    "2 | {\\bf A}^{\\star} - {\\bf A} ({k_n}) |\n",
    "\\frac{\\partial |A^{\\star} - A (k_n)|}{\\partial k_n}.\n",
    "$\n",
    "\n",
    "Finally,\n",
    "$\\frac{\\partial^2 g(k_n)}{\\partial^2 k_n} =\n",
    "2\n",
    "\\left( \\frac{\\partial |A^{\\star} - A (k_n)|}{\\partial k_n} \\right) ^2 \n",
    "- 2 | {\\bf A}^{\\star} - {\\bf A} ({k_n}) | \\frac{\\partial^2 |A^{\\star} - A (k_n)|}{\\partial^2 k_n}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca717f40",
   "metadata": {},
   "source": [
    "We claim that\n",
    "$\n",
    "\\frac{\\partial^2 |A (k_n)|}{\\partial^2 k_n} = 0.\n",
    "$\n",
    "This follows from the fact that $k_n$ occurs in only one column since it is associated with only one state variable.\n",
    "As such, $k_n$ occurs in a product term of the determinant with a power of either 0 or 1.\n",
    "The second derivative of such terms is 0; and the first derivative of terms without $k_n$ is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e23e9",
   "metadata": {},
   "source": [
    "**Issues**\n",
    "1. What does this mean since $A^{\\star}$ is unknown?\n",
    "\n",
    "1. Let ${\\bf x}^{\\star}$ be the solution to $\\dot{{\\bf x}}^{\\star} = {\\bf A}^{\\star} {\\bf x}^{\\star}$.\n",
    "We have ${\\bf x}^{\\star}$.\n",
    "So, we should consider\n",
    "$h(k_n) = | {\\bf x} (0) e^{ {\\bf A}^{\\star} t } - {\\bf x}(0) e^{ {\\bf A (k_n)} t } | \\approx  \n",
    "| {\\bf x}^{\\star} (t)  - {\\bf A} (k_n) {\\bf x} (0) t |.$\n",
    "Can make an argument for the $L1$ norm and that the differences are minimized if the matrices converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e64b2a",
   "metadata": {},
   "source": [
    "## Convergence of Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a6613",
   "metadata": {},
   "source": [
    "Let $A({\\bf k})$ be the parameterization of the a linear system with true parameters ${\\bf k}^{\\star}$.\n",
    "Define ${\\bf x}^{\\star}$ as the true solution, and ${\\bf r}(t, {\\bf k}) =  {\\bf x}^{\\star} - e^{A({\\bf k}) t} x(0)$.\n",
    "Let\n",
    "$g({\\bf k}) = \\int {\\bf r}^T (t, {\\bf k}) {\\bf r} (t, {\\bf k})dt$.\n",
    "Show that $g({\\bf k})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4ece4",
   "metadata": {},
   "source": [
    "1. It suffices to show that $g$ is convex in each $k_n$ for each $t$.\n",
    "1. $e^{A({\\bf k} t} x(0) = \\left( \\sum_{n=0}^{\\infty} \\frac{ \\left( A({\\bf k}) \\right)^n t^n}{n!} \\right) {\\bf x}(0)$.\n",
    "\n",
    "1. ${\\bf r}(t) = \\left( \\sum_{n=0}^{\\infty} \\frac{ \\left( {\\bf A}^{\\star} \\right)^n - \\left( {\\bf  A}({\\bf k}) \\right)^n t^n} {n!} \n",
    "\\right) {\\bf x}(0)$.\n",
    "\n",
    "1. Let $a_{ij}$ be the $i,j$ element of $\\left( {\\bf A} (k) \\right) ^n$. Note that $a_{ij} = c_m k^m + \\cdots + c_0 = P_{ij}^m(k)$,\n",
    "where $m \\leq 2^{n-1}$.\n",
    "Define $p({\\bf A} (k)) = \\{ P_{ij}^m \\}$.\n",
    "\n",
    "1. Let $P^m(k)$ be an $m$-th order polynomial in $k$.\n",
    "   1. $\\frac{\\partial P^m(k)}{\\partial k} = P^{m-1}(k)$.\n",
    "   1. $sign(c_n^m) = sign(c_{n-1}^{m-1})$ for $n > 0$; otherwise, $sign(c_n) = 0$.\n",
    "   1. The order of the polynomial of $k$ in ${\\bf A}(k)$ is the same as that for\n",
    "   ${\\bf A}(k) {\\bf x}$, for a vector ${\\bf x}$ that does not contain $k$.\n",
    "   1. The order of the polynomial of $k$ in ${\\bf A}(k)$ is the same as that for\n",
    "   ${\\bf A}^{\\star} - {\\bf A}(k)$, for a matrix ${\\bf A}^{\\star}$ that does not contain $k$.\n",
    "   1. The order of the polynomial of $k$ in ${\\bf A}_1(k) + {\\bf A}_2(k)$ is\n",
    "   the max of the order of the polynomials for ${\\bf A}_1(k), {\\bf A}_2(k)$.\n",
    "   \n",
    "1. \n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 {\\bf r}^T (t, k) {\\bf r} (t, k)}{\\partial^2 k} \n",
    "& = &\n",
    "   \\frac {\\partial^2 {\\bf r}^T (t, k)} {\\partial^2 k} {\\bf r} (t, k) +\n",
    "   2 \\frac {\\partial {\\bf r}^T (t, k)} {\\partial k}  \\frac {\\partial {\\bf r} (t, k)} {\\partial k} +\n",
    "   {\\bf r}^T (t, k)  \\frac {\\partial^2 {\\bf r} (t, k)} {\\partial^2 k} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "1. A first order Taylor series approximation of $k$ is convex because:\n",
    "   1. ${\\bf A (k)} = \\{P^1_{ij} \\}$\n",
    "   1. The second derivatives of ${\\bf r}(t, k) \\}$ are 0 and the first derviative is squared, and so is positive.\n",
    "   \n",
    "1. If ${\\bf A}$ is a decompled system, then ${\\bf A}$ a diagnonal matrix. This could allow for some reduction in dimensionality for parameter estimation.\n",
    "We still may have high order polynomials in $k$ for $a_{ii}$.\n",
    "\n",
    "1. The key consideration is the sign of terms in the polynomials.\n",
    "    1. If they are always positive, then we get convexity since\n",
    "terms without $k$ disappear because of the derivative.\n",
    "    1. If terms of the same sign are multiplied, then the result is positive.\n",
    "    1. Terms on the diagnonal tend to be negative since they are degradation rates for a species.\n",
    "    \n",
    "1. For $A$, and $N \\times N$ matrix with $k$ only in $a_{ij}$, find $m$ such that $A^{m+1}$ has the same cells with $k$ as $A^m$.\n",
    "\n",
    "1. Suppose $X_i$ is the reactant in a reaction that produces $X_j$ with mass action kinetics and constant $k$.\n",
    "Then, $k$ has a positive sign in $a_{ji}$ and a negative sign in $a_{ij}$.\n",
    "Let $B = A^2$.\n",
    "Then, \n",
    "   * $b_{ii} = b_{jj}$ has the term $-k^2$\n",
    "   * $b_{ik}$ has the term $-k$\n",
    "   * $b_{kj}$ has the term $k$\n",
    "\n",
    "1. Let $\\{ a^m_{ij}\\} = {\\bf A}^m (k)$.\n",
    "The sign of $a^m_{ij}$ depends on the sign of non-$k$ terms in the matrix.\n",
    "In particular, the negative diagnonal.\n",
    "\n",
    "1. The two main considerations in this analysis is the order of $P^m_{ij}(k)$ and the sign of the terms in the polynomial.\n",
    "\n",
    "1. Another strategy is to construct an approximation to ${\\bf x} (k)$ based on parts of different matrices in the\n",
    "exact solution such that all terms with $k$ are positive.\n",
    "This will yield a convex function.\n",
    "The construction of this approximation depends on ${\\bf A}$, where $-k$ occurs in $\\left( {\\bf A} (k) \\right)^m$.\n",
    "This seems feasible to construct if:\n",
    "\n",
    "   * $ \\{ a_{ij} \\} $ is the some of terms of the same sign and\n",
    "   * $k$ occurs only in only a few locations: $i,i$; $i,j$ and $j,i$; $i,j$, $j_1, i, \\cdots, j_k, i$ (branching)\n",
    "  \n",
    "1. A second order approximation is convex since the only terms with $k$ that are second order are positive as long as their is no order 2 feedback (e.g., $X \\rightarrow Y \\rightarrow X$.\n",
    "Can I generalize this to doing an approximation of order $m$ if there is no feedback of order $m \\leq N$?\n",
    "\n",
    "1. Assuming that only $x_1$ has a non-zero initial condition, then ${\\bf x}(0)$ results in just the first column being selected. However, we may need more terms in the approximation for it to be of reasonable quality.\n",
    "\n",
    "1. **Observation**. Let $r(t, {\\bf k}) = {\\bf x}^* (t) - \\left( {\\bf I} + {\\bf A}({\\bf k}) \\right) {\\bf x} (0) t$. Then, $min~ g({\\bf k}) = min \\int {\\bf r}^T (t, {\\bf k}) {\\bf r} (t, {\\bf k})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d783f0c",
   "metadata": {},
   "source": [
    "## Algorithm 1: Linear Approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6105d5",
   "metadata": {},
   "source": [
    "Below outlines an algorithm for parameter estimation based on the observation that a first order approximation of a linear system has a convex optimization objective for $l^2$.\n",
    "\n",
    "1. Given ${\\bf x}^{\\star} (t), {\\bf v}({\\bf k}, {\\bf x})$, the vector of reaction rates.\n",
    "\n",
    "1. Symbolically calculate ${\\bf V}( {\\bf k}, {\\bf x}) = \\frac{\\partial v}{\\partial x}$. ${\\bf A}_i ({\\bf k}) = {\\bf V} ({\\bf k}) |_{t = t_i}$. Note that ${\\bf A}({\\bf k})$ has unevaluated ${\\bf k}$.\n",
    "\n",
    "1. ${\\bf d} (i+1, i) = {\\bf \\delta x}^{\\star}_{t_{i+1}} - {\\bf A}_i ({\\bf k}) {\\bf x}^{\\star}_i (t_{i +1} - t_{i})$,\n",
    "where $\\delta {\\bf x}^{\\star}_i = {\\bf x}^{\\star}_{t_{i+1}} - {\\bf x}^{\\star}_{t_{i}}$.\n",
    "\n",
    "1. $r_i ({\\bf k}) = {\\bf d} (i-1, i) + {\\bf d}(i+1, i)$. This is a two sided difference. It could be redefined as a one sided difference.\n",
    "\n",
    "1. $g({\\bf k}) = \\sum_i r^T_i ({\\bf k}) r_i ({\\bf k})$\n",
    "1. Find ${\\bf k}$ that minimizes $g({\\bf k})$. Note that since each individual problem is convex, their sum is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd070b80",
   "metadata": {},
   "source": [
    "**Issues**\n",
    "1. Accuracy of the linear approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864a67e",
   "metadata": {},
   "source": [
    "## Algorithm 2: Decoupling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee55c90",
   "metadata": {},
   "source": [
    "Assume that time courses are available for all floating species.\n",
    "\n",
    "    for X in states\n",
    "      fit the parameters for the state equation for X\n",
    "    reconcile differences in parameter fits since the same parameter may be present in different state equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123b3ba",
   "metadata": {},
   "source": [
    "**Appeal**\n",
    "1. No approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0afcc",
   "metadata": {},
   "source": [
    "**Issues**\n",
    "1. Must have measurements of all state variables\n",
    "1. Must reconcile differences in parameter fits (unlike S-systems, where parameters exist in only one state equation). An\n",
    "example of this is the production of a species and its consumption are in different state equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91a908",
   "metadata": {},
   "source": [
    "## Analysis of the ${\\bf A}$ Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Amat = sympy.symbols(\"Amat\")\n",
    "Amat = sympy.Matrix( [ [1,-1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "Amat**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b784a9",
   "metadata": {},
   "source": [
    "# Arbitray Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a0b2b",
   "metadata": {},
   "source": [
    "## Glycolytic Oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterDct = {\"J1_Ki\": 1, \"J1_k1\": 550}\n",
    "analyzer = SurfaceAnalyzer(GMODEL, parameterDct)\n",
    "for scale in [0.0004]:\n",
    "    analyzer.runExperiments(1.0, 50)\n",
    "    analyzer.plotSurface(scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537af03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterDct = {\"J1_Ki\": 1, \"J2_k\": 9.8}\n",
    "analyzer = SurfaceAnalyzer(GMODEL, parameterDct)\n",
    "for scale in [0.03]:\n",
    "    analyzer.runExperiments(0.2, 50)\n",
    "    analyzer.plotSurface(scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5dc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterDct = {\"J1_k1\": 550, \"J2_k\": 9.8}\n",
    "analyzer = SurfaceAnalyzer(GMODEL, parameterDct)\n",
    "for scale in [0.0005]:\n",
    "    analyzer.runExperiments(1.0, 40)\n",
    "    analyzer.plotSurface(scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285bc583",
   "metadata": {},
   "source": [
    "# FFL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sympy.eye(3)\n",
    "b = sympy.Matrix([10, 0, 0])\n",
    "system = A, b\n",
    "su.addSymbols(\"x0 x1 x2\")\n",
    "x = sympy.Matrix([x0, x1, x2])\n",
    "sympy.linsolve(system, *x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "su.addSymbols(\"k1 k2 k3\")\n",
    "Amat = sympy.Matrix( [[0, 0, 0, 0], [k1, -k2, 0, 0], [0, k2, -k3, 0], [0, 0, 0, 0]])\n",
    "Amat.eigenvects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6488a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z, k1, k2 = sympy.symbols(\"X Y Z k1 k2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67785905",
   "metadata": {},
   "outputs": [],
   "source": [
    "aMat = sympy.Matrix([ [0, 0, 0, 0], [1, -(1 + k2), 0, 0], [0, 1, -k1, 0], [0, k2, k1, -1]])\n",
    "aMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ce20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialVec = sympy.Matrix([1, 0, 0, 0])\n",
    "initialVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LTIModel(aMat, initialVec)\n",
    "model.solve()\n",
    "sympy.simplify(model.solutionVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.expand(model.solutionVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b758b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "su.addSymbols(\"k0 k1 k2\")\n",
    "SUBS = {k0: 1, k1: 1, k2: 3}\n",
    "model.plot(0, 10, 100, subs=SUBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tellurium as te\n",
    "MODEL = \"\"\"\n",
    "$X0 -> X; 1*X0\n",
    "X -> Y; k0*X\n",
    "Y -> Z; k1*Y\n",
    "X -> Z; k2*X\n",
    "Z -> ; 1*Z\n",
    "\n",
    "$X0 = 1\n",
    "k0 = 1\n",
    "k1 = 2\n",
    "k2 = 3\n",
    "\"\"\"\n",
    "rr = te.loada(MODEL)\n",
    "rr.plot(rr.simulate(0, 10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.exp(-3+2j)\n",
    "np.angle(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490b3cd",
   "metadata": {},
   "source": [
    "# Solving Differential Equations with Complex Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c413a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 2\\\\-10 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  1, 2],\n",
       "[-10, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aMat = sympy.Matrix([ [1, 2], [-10, 1]])\n",
    "aMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6897df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0 - 4.47213595499958*I: 1, 1.0 + 4.47213595499958*I: 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenDct = {sympy.N(k): v for k, v in aMat.eigenvals().items()}\n",
    "eigenDct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e74ae6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aMat.eigenvects()\n",
    "eigenVec1 = result[0][2][0]\n",
    "eigenval1 = result[0][0]\n",
    "eigenVec2 = result[1][2][0]\n",
    "eigenval2 = result[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1e1a593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 + \\frac{\\sqrt{5} i}{5}\\\\1 - 2 \\sqrt{5} i\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2 + sqrt(5)*I/5],\n",
       "[1 - 2*sqrt(5)*I]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aMat * eigenVec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e4f5fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 + \\frac{\\sqrt{5} i}{5}\\\\1 - 2 \\sqrt{5} i\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2 + sqrt(5)*I/5],\n",
       "[1 - 2*sqrt(5)*I]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.simplify(eigenVec1 * eigenval1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1d3835f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Matrix([\n",
       " [sqrt(5)*I/5],\n",
       " [          1]]),\n",
       " Matrix([\n",
       " [-sqrt(5)*I/5],\n",
       " [           1]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenVec1, eigenVec2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7d7a6",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6eba4",
   "metadata": {},
   "source": [
    "1. Objectives\n",
    "   1. Research - relate that nature of the fitting surface to the chemical network. Show in which cases the fitting surface is convex. Provide insights into strategies for fitting.\n",
    "   1. Teaching - provide insights into what is happening with the different fitting algorithms by animating their steps.\n",
    "   \n",
    "1. Research directions\n",
    "   1. Fitting surfaces by network type: linear, branched, feedback.\n",
    "   1. Preliminary insights\n",
    "      1. Detecting easily fit networks\n",
    "      1. When do starting positions matter. For which parameters (because of combinatorics). How narrow parameter ranges (to reduce combinatorics).\n",
    "      \n",
    "1. A strategy for a linear chain (and maybe more general networks) is to first optimize the ratio of species concentrations.\n",
    "But which ratios?\n",
    "\n",
    "1. One strategy is to synthesize first order systems and solve them separately or recursively. Consider a linear system.\n",
    "   1. Remove the constant term, since this is steady state.\n",
    "   1. Integrate the rest.\n",
    "   1. If the result is 0, stop.\n",
    "   1. Otherwise, go to (a)\n",
    "   \n",
    "   Solve the last system found, since this is the high order system. Use these constants, and repeat.\n",
    "\n",
    "1. To do\n",
    "    1. Show simulations for different points in the curve, relating the simulation results to $R^2$.\n",
    "    1. Look at longer linear chains.\n",
    "    1. Examine Wolf model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
